{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_You are currently looking at **version 1.1** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-data-analysis/resources/0dhYG) course resource._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Hypothesis Testing\n",
    "This assignment requires more individual learning than previous assignments - you are encouraged to check out the [pandas documentation](http://pandas.pydata.org/pandas-docs/stable/) to find functions or methods you might not have used yet, or ask questions on [Stack Overflow](http://stackoverflow.com/) and tag them as pandas and python related. And of course, the discussion forums are open for interaction with your peers and the course staff.\n",
    "\n",
    "Definitions:\n",
    "* A _quarter_ is a specific three month period, Q1 is January through March, Q2 is April through June, Q3 is July through September, Q4 is October through December.\n",
    "* A _recession_ is defined as starting with two consecutive quarters of GDP decline, and ending with two consecutive quarters of GDP growth.\n",
    "* A _recession bottom_ is the quarter within a recession which had the lowest GDP.\n",
    "* A _university town_ is a city which has a high percentage of university students compared to the total population of the city.\n",
    "\n",
    "**Hypothesis**: University towns have their mean housing prices less effected by recessions. Run a t-test to compare the ratio of the mean price of houses in university towns the quarter before the recession starts compared to the recession bottom. (`price_ratio=quarter_before_recession/recession_bottom`)\n",
    "\n",
    "The following data files are available for this assignment:\n",
    "* From the [Zillow research data site](http://www.zillow.com/research/data/) there is housing data for the United States. In particular the datafile for [all homes at a city level](http://files.zillowstatic.com/research/public/City/City_Zhvi_AllHomes.csv), ```City_Zhvi_AllHomes.csv```, has median home sale prices at a fine grained level.\n",
    "* From the Wikipedia page on college towns is a list of [university towns in the United States](https://en.wikipedia.org/wiki/List_of_college_towns#College_towns_in_the_United_States) which has been copy and pasted into the file ```university_towns.txt```.\n",
    "* From Bureau of Economic Analysis, US Department of Commerce, the [GDP over time](http://www.bea.gov/national/index.htm#gdp) of the United States in current dollars (use the chained value in 2009 dollars), in quarterly intervals, in the file ```gdplev.xls```. For this assignment, only look at GDP data from the first quarter of 2000 onward.\n",
    "\n",
    "Each function in this assignment below is worth 10%, with the exception of ```run_ttest()```, which is worth 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OH': 'Ohio',\n",
       " 'KY': 'Kentucky',\n",
       " 'AS': 'American Samoa',\n",
       " 'NV': 'Nevada',\n",
       " 'WY': 'Wyoming',\n",
       " 'NA': 'National',\n",
       " 'AL': 'Alabama',\n",
       " 'MD': 'Maryland',\n",
       " 'AK': 'Alaska',\n",
       " 'UT': 'Utah',\n",
       " 'OR': 'Oregon',\n",
       " 'MT': 'Montana',\n",
       " 'IL': 'Illinois',\n",
       " 'TN': 'Tennessee',\n",
       " 'DC': 'District of Columbia',\n",
       " 'VT': 'Vermont',\n",
       " 'ID': 'Idaho',\n",
       " 'AR': 'Arkansas',\n",
       " 'ME': 'Maine',\n",
       " 'WA': 'Washington',\n",
       " 'HI': 'Hawaii',\n",
       " 'WI': 'Wisconsin',\n",
       " 'MI': 'Michigan',\n",
       " 'IN': 'Indiana',\n",
       " 'NJ': 'New Jersey',\n",
       " 'AZ': 'Arizona',\n",
       " 'GU': 'Guam',\n",
       " 'MS': 'Mississippi',\n",
       " 'PR': 'Puerto Rico',\n",
       " 'NC': 'North Carolina',\n",
       " 'TX': 'Texas',\n",
       " 'SD': 'South Dakota',\n",
       " 'MP': 'Northern Mariana Islands',\n",
       " 'IA': 'Iowa',\n",
       " 'MO': 'Missouri',\n",
       " 'CT': 'Connecticut',\n",
       " 'WV': 'West Virginia',\n",
       " 'SC': 'South Carolina',\n",
       " 'LA': 'Louisiana',\n",
       " 'KS': 'Kansas',\n",
       " 'NY': 'New York',\n",
       " 'NE': 'Nebraska',\n",
       " 'OK': 'Oklahoma',\n",
       " 'FL': 'Florida',\n",
       " 'CA': 'California',\n",
       " 'CO': 'Colorado',\n",
       " 'PA': 'Pennsylvania',\n",
       " 'DE': 'Delaware',\n",
       " 'NM': 'New Mexico',\n",
       " 'RI': 'Rhode Island',\n",
       " 'MN': 'Minnesota',\n",
       " 'VI': 'Virgin Islands',\n",
       " 'NH': 'New Hampshire',\n",
       " 'MA': 'Massachusetts',\n",
       " 'GA': 'Georgia',\n",
       " 'ND': 'North Dakota',\n",
       " 'VA': 'Virginia'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use this dictionary to map state names to two letter acronyms\n",
    "states = {'OH': 'Ohio', 'KY': 'Kentucky', 'AS': 'American Samoa', 'NV': 'Nevada', 'WY': 'Wyoming', 'NA': 'National', 'AL': 'Alabama', 'MD': 'Maryland', 'AK': 'Alaska', 'UT': 'Utah', 'OR': 'Oregon', 'MT': 'Montana', 'IL': 'Illinois', 'TN': 'Tennessee', 'DC': 'District of Columbia', 'VT': 'Vermont', 'ID': 'Idaho', 'AR': 'Arkansas', 'ME': 'Maine', 'WA': 'Washington', 'HI': 'Hawaii', 'WI': 'Wisconsin', 'MI': 'Michigan', 'IN': 'Indiana', 'NJ': 'New Jersey', 'AZ': 'Arizona', 'GU': 'Guam', 'MS': 'Mississippi', 'PR': 'Puerto Rico', 'NC': 'North Carolina', 'TX': 'Texas', 'SD': 'South Dakota', 'MP': 'Northern Mariana Islands', 'IA': 'Iowa', 'MO': 'Missouri', 'CT': 'Connecticut', 'WV': 'West Virginia', 'SC': 'South Carolina', 'LA': 'Louisiana', 'KS': 'Kansas', 'NY': 'New York', 'NE': 'Nebraska', 'OK': 'Oklahoma', 'FL': 'Florida', 'CA': 'California', 'CO': 'Colorado', 'PA': 'Pennsylvania', 'DE': 'Delaware', 'NM': 'New Mexico', 'RI': 'Rhode Island', 'MN': 'Minnesota', 'VI': 'Virgin Islands', 'NH': 'New Hampshire', 'MA': 'Massachusetts', 'GA': 'Georgia', 'ND': 'North Dakota', 'VA': 'Virginia'}\n",
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "umich_part_id": "021",
    "umich_partlist_id": "004"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Auburn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Florence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jacksonville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Livingston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>Stevens Point</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>Waukesha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>Whitewater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>Laramie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>567 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "0          Alabama\n",
       "1           Auburn\n",
       "2         Florence\n",
       "3     Jacksonville\n",
       "4       Livingston\n",
       "..             ...\n",
       "562  Stevens Point\n",
       "563       Waukesha\n",
       "564     Whitewater\n",
       "565        Wyoming\n",
       "566        Laramie\n",
       "\n",
       "[567 rows x 1 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_list_of_university_towns():\n",
    "    '''Returns a DataFrame of towns and the states they are in from the \n",
    "    university_towns.txt list. The format of the DataFrame should be:\n",
    "    DataFrame( [ [\"Michigan\", \"Ann Arbor\"], [\"Michigan\", \"Yipsilanti\"] ], \n",
    "    columns=[\"State\", \"RegionName\"]  )\n",
    "    \n",
    "    The following cleaning needs to be done:\n",
    "\n",
    "    1. For \"State\", removing characters from \"[\" to the end.\n",
    "    2. For \"RegionName\", when applicable, removing every character from \" (\" to the end.\n",
    "    3. Depending on how you read the data, you may need to remove newline character '\\n'. '''\n",
    "  \n",
    "      # First we read the txt file, we have to use new line (\\n) as separator. \n",
    "    university_towns = pd.read_csv('university_towns.txt', sep='\\n', header = None )\n",
    "    \n",
    "    # Then we clean the data by removing parenthesis and squared brackets\n",
    "    # expand = True splits the string into different columns.\n",
    "    for col in university_towns:\n",
    "        university_towns[col] = university_towns[col].str.split('(',expand=True)[0].str.split('[', expand=True)[0].str.rstrip()\n",
    "     \n",
    "    \n",
    "      # We have to find now the states in the data, to use them later to create a Series with them, \n",
    "      # which will be the first column with the States names. To do so, first get them from the values in \n",
    "      # the given dictionary (states), then we sort alphabetically the states (because they appear alphabetically in \n",
    "      # our df university_towns) and we remove those states which are not in our dataframe (because these states do not have uni towns).\n",
    "    stts = sorted(list(states.values()))\n",
    "    for st in stts:\n",
    "        if st not in university_towns.iloc[:,0].tolist():\n",
    "            stts.remove(st)\n",
    "    \n",
    "      # Now it is time to create the list (States) which will contain the States repeated as many times as uni towns they have. \n",
    "      # To do so we iterate through a for loop and we get the index of the next state in university_towns.iloc[:,0] (n+1) and\n",
    "      # substract it from the index of the State we are checking (n). The result of this substraction is the number of uni towns \n",
    "      # from the n-State (+1 extra for the State itself, which is not a town, so we will have to remove those extra lines later)\n",
    "    States = []\n",
    "    for n in range(0,len(stts)-1):\n",
    "        i = university_towns.index[university_towns.iloc[:,0]==stts[n]].tolist()[0]\n",
    "        j = university_towns.index[university_towns.iloc[:,0]==stts[n+1]].tolist()[0]\n",
    "        x=j-i\n",
    "        States = States + list([stts[n]]*x)\n",
    "      # There is a problem in the loop with the last State, so we have to use this trick to complete the States list. \n",
    "      # Extra is the difference between the length of the df column university_towns.iloc[:,0] and the length of Sates which \n",
    "      # is missing the last State so far. This gives us the number of times the last State has to be repeated and we add it to \n",
    "      # the States list to complete it\n",
    "    extra = len(university_towns.iloc[:,0].tolist())-len(States)\n",
    "    States = States + list([stts[-1]]*extra)\n",
    "\n",
    "      # We convert now the States list to a Series, add it to our df, rename the columns and finally reorder it\n",
    "    university_towns['State'] = pd.Series(States)\n",
    "    university_towns.columns = [\"RegionName\", \"State\"]\n",
    "    university_towns = university_towns[['State', \"RegionName\"]]\n",
    "\n",
    "      # Remember we have extra lines, where the States appear as towns. We have to remove those, by using the drop function\n",
    "    university_towns.drop(university_towns[university_towns[\"State\"]==university_towns[\"RegionName\"]].index, inplace = True)\n",
    "    university_towns.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    return university_towns\n",
    "\n",
    "get_list_of_university_towns()\n",
    "university_towns = pd.read_csv('university_towns.txt', sep='\\n', header = None )\n",
    "for col in university_towns:\n",
    "        university_towns[col] = university_towns[col].str.split('(',expand=True)[0].str.split('[', expand=True)[0].str.rstrip()\n",
    "\n",
    "university_towns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "umich_part_id": "022",
    "umich_partlist_id": "004"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2008q3'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_recession_start():\n",
    "        # We read the gdp excel, skipping the columns and rows we will not use. We rename the columns too with more meaningful names\n",
    "    gdp = pd.read_excel('gdplev.xls', header = None, skiprows = range(0, 8), usecols = [4,5,6])\n",
    "    gdp.columns = ['Quarter', 'GDP in billions of current dollars', 'GDP in billions of chained 2009 dollars']\n",
    "\n",
    "        # We will use data from the first Quarter of 2000 on, so we need the index of this row, and then we use the dataframe from \n",
    "        # this row on. We also need to reset the row index to have it beginning at 0\n",
    "    gdp.index[gdp['Quarter']=='2000q1'].tolist()[0]\n",
    "    gdp = gdp.iloc[gdp.index[gdp['Quarter']=='2000q1'].tolist()[0]:,:]\n",
    "    gdp.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # We create another column to check the porcentual variation of gdP\n",
    "    gdp['GDP in billions of chained 2009 dollars variation (%)'] = gdp.loc[:,'GDP in billions of chained 2009 dollars'].pct_change() * 100\n",
    "\n",
    "        # With a for loop we check the row where the recession starts (a recession starts when there are two consecutive quarters \n",
    "        # of GDP decline)\n",
    "    recession_index = []\n",
    "    for n in range(0,len(gdp.loc[:,'GDP in billions of chained 2009 dollars variation (%)'].tolist())-1):\n",
    "        if (gdp.loc[n,'GDP in billions of chained 2009 dollars variation (%)']<0) & (gdp.loc[n+1,'GDP in billions of chained 2009 dollars variation (%)']<0):\n",
    "            recession_index.append(n)\n",
    "\n",
    "        # If there are several consecutive quarters with negative porcentual variation, we just need the one when it started, so we remove the following ones\n",
    "        # With the first for loop we check the consecutive indexes and we create a list (to_remove) which we will use later in another loop to obtain just the \n",
    "        # quarter when the recession started as explained before\n",
    "    to_remove = []\n",
    "    for n in range(0,2):\n",
    "        if recession_index[n+1]==recession_index[n]+1:\n",
    "            to_remove.append(recession_index[n+1])\n",
    "    recession_index = [x for x in recession_index if x not in to_remove]\n",
    "\n",
    "        # We can now obtain the qurter when the recssion started\n",
    "    return gdp.iloc[recession_index[0],0]\n",
    "    \n",
    "get_recession_start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "umich_part_id": "023",
    "umich_partlist_id": "004"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2009q4'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_recession_end():\n",
    "    \n",
    "    '''Returns the year and quarter of the recession end as a \n",
    "    string value in a format such as 2005q3'''\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # -------------------- WE WILL REUSE PART OF THE CODE FROM THE PREVIOUS FUNTION -------------------------------------------------------------------\n",
    "    # ----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # We read the gdp excel, skipping the columns and rows we will not use. We rename the columns too with more meaningful names\n",
    "    gdp = pd.read_excel('gdplev.xls', header = None, skiprows = range(0, 8), usecols = [4,5,6])\n",
    "    gdp.columns = ['Quarter', 'GDP in billions of current dollars', 'GDP in billions of chained 2009 dollars']\n",
    "        # We will use data from the first Quarter of 2000 on, so we need the index of this row, and then use the dataframe from \n",
    "        # this row on. We also need to reset the row index to have it beginning at 0\n",
    "    gdp.index[gdp['Quarter']=='2000q1'].tolist()[0]\n",
    "    gdp = gdp.iloc[gdp.index[gdp['Quarter']=='2000q1'].tolist()[0]:,:]\n",
    "    gdp.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # We create another column to check the porcentual variation of gdP\n",
    "    gdp['GDP in billions of chained 2009 dollars variation (%)'] = gdp.loc[:,'GDP in billions of chained 2009 dollars'].pct_change() * 100\n",
    "\n",
    "        # With a for loop we check the row where the recession starts (a recession starts when there are two consecutive quarters \n",
    "        # of GDP decline)\n",
    "    recession_index = []\n",
    "    for n in range(0,len(gdp.loc[:,'GDP in billions of chained 2009 dollars variation (%)'].tolist())-1):\n",
    "        if (gdp.loc[n,'GDP in billions of chained 2009 dollars variation (%)']<0) & (gdp.loc[n+1,'GDP in billions of chained 2009 dollars variation (%)']<0):\n",
    "            recession_index.append(n)\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # -----------------------------HERE FINISHES THE CODE REUSE----------------------------------------------------------------------------------------\n",
    "    # ----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # We use the same logic in the for loop to find the row where the recession ends (a recession ends when there are two consecutive quarters \n",
    "        # of GDP increase). In this case we need to break the loop after finding the row\n",
    "    end_of_recession_index = []\n",
    "    for n in range(recession_index[0],len(gdp.loc[:,'GDP in billions of chained 2009 dollars variation (%)'].tolist())-1):\n",
    "        if (gdp.loc[n,'GDP in billions of chained 2009 dollars variation (%)']>0) & (gdp.loc[n+1,'GDP in billions of chained 2009 dollars variation (%)']>0):\n",
    "            end_of_recession_index.append(n+1)\n",
    "            break\n",
    "\n",
    "        # We can now obtain the qurter when the recssion finished\n",
    "    return gdp.iloc[end_of_recession_index[0],0]\n",
    "    \n",
    "get_recession_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "umich_part_id": "024",
    "umich_partlist_id": "004"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2009q2'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_recession_bottom():\n",
    "    '''Returns the year and quarter of the recession bottom time as a \n",
    "    string value in a format such as 2005q3'''\n",
    "        \n",
    "    # ----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # -------------------- WE WILL REUSE PART OF THE CODE FROM THE PREVIOUS FUNTION -------------------------------------------------------------------\n",
    "    # ----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "        # We read the gdp excel, skipping the columns and rows we will not use. We rename the columns too with more meaningful names\n",
    "    gdp = pd.read_excel('gdplev.xls', header = None, skiprows = range(0, 8), usecols = [4,5,6])\n",
    "    gdp.columns = ['Quarter', 'GDP in billions of current dollars', 'GDP in billions of chained 2009 dollars']\n",
    "        # We will use data from the first Quarter of 2000 on, so we need the index of this row, and then use the dataframe from \n",
    "        # this row on. We also need to reset the row index to have it beginning at 0\n",
    "    gdp.index[gdp['Quarter']=='2000q1'].tolist()[0]\n",
    "    gdp = gdp.iloc[gdp.index[gdp['Quarter']=='2000q1'].tolist()[0]:,:]\n",
    "    gdp.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # We create another column to check the porcentual variation of gdP\n",
    "    gdp['GDP in billions of chained 2009 dollars variation (%)'] = gdp.loc[:,'GDP in billions of chained 2009 dollars'].pct_change() * 100\n",
    "\n",
    "        # With a for loop we check the row where the recession starts (a recession starts when there are two consecutive quarters \n",
    "        # of GDP decline)\n",
    "    recession_index = []\n",
    "    for n in range(0,len(gdp.loc[:,'GDP in billions of chained 2009 dollars variation (%)'].tolist())-1):\n",
    "        if (gdp.loc[n,'GDP in billions of chained 2009 dollars variation (%)']<0) & (gdp.loc[n+1,'GDP in billions of chained 2009 dollars variation (%)']<0):\n",
    "            recession_index.append(n)\n",
    "        # With a for loop we check the row where the recession starts (a recession starts when there are two consecutive quarters \n",
    "        # of GDP decline)\n",
    "    end_of_recession_index = []\n",
    "    for n in range(recession_index[0],len(gdp.loc[:,'GDP in billions of chained 2009 dollars variation (%)'].tolist())-1):\n",
    "        if (gdp.loc[n,'GDP in billions of chained 2009 dollars variation (%)']>0) & (gdp.loc[n+1,'GDP in billions of chained 2009 dollars variation (%)']>0):\n",
    "            end_of_recession_index.append(n+1)\n",
    "            break\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    # -----------------------------HERE FINISHES THE CODE REUSE----------------------------------------------------------------------------------------\n",
    "    # ----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "         \n",
    "        # Now we reduce the gdp df to the recession rows, we find the minimum of GDP in billions of chained 2009 dollars within\n",
    "        # this period, the row index for this minimum and finally the quarter when it happened\n",
    "    recession_df = gdp.iloc[recession_index[0]:end_of_recession_index[0]+1,:]\n",
    "    min_GDP_in_recession = recession_df['GDP in billions of chained 2009 dollars'].min()\n",
    "    recession_bottom_index = gdp.index[gdp['GDP in billions of chained 2009 dollars']==min_GDP_in_recession].tolist()[0]\n",
    "    gdp.iloc[recession_bottom_index,0]\n",
    "    return gdp.iloc[recession_bottom_index,0]\n",
    "\n",
    "get_recession_bottom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "umich_part_id": "025",
    "umich_partlist_id": "004"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>2000q1</th>\n",
       "      <th>2000q2</th>\n",
       "      <th>2000q3</th>\n",
       "      <th>2000q4</th>\n",
       "      <th>2001q1</th>\n",
       "      <th>2001q2</th>\n",
       "      <th>2001q3</th>\n",
       "      <th>2001q4</th>\n",
       "      <th>2002q1</th>\n",
       "      <th>2002q2</th>\n",
       "      <th>...</th>\n",
       "      <th>2014q2</th>\n",
       "      <th>2014q3</th>\n",
       "      <th>2014q4</th>\n",
       "      <th>2015q1</th>\n",
       "      <th>2015q2</th>\n",
       "      <th>2015q3</th>\n",
       "      <th>2015q4</th>\n",
       "      <th>2016q1</th>\n",
       "      <th>2016q2</th>\n",
       "      <th>2016q3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <th>RegionName</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Alabama</th>\n",
       "      <th>Adamsville</th>\n",
       "      <td>69033.333333</td>\n",
       "      <td>69166.666667</td>\n",
       "      <td>69800.000000</td>\n",
       "      <td>71966.666667</td>\n",
       "      <td>73466.666667</td>\n",
       "      <td>74000.000000</td>\n",
       "      <td>73333.333333</td>\n",
       "      <td>73100.000000</td>\n",
       "      <td>73333.333333</td>\n",
       "      <td>73133.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>77066.666667</td>\n",
       "      <td>75966.666667</td>\n",
       "      <td>71900.000000</td>\n",
       "      <td>71666.666667</td>\n",
       "      <td>73033.333333</td>\n",
       "      <td>73933.333333</td>\n",
       "      <td>73866.666667</td>\n",
       "      <td>74166.666667</td>\n",
       "      <td>74933.333333</td>\n",
       "      <td>74700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alabaster</th>\n",
       "      <td>122133.333333</td>\n",
       "      <td>123066.666667</td>\n",
       "      <td>123166.666667</td>\n",
       "      <td>123700.000000</td>\n",
       "      <td>123233.333333</td>\n",
       "      <td>125133.333333</td>\n",
       "      <td>127766.666667</td>\n",
       "      <td>127200.000000</td>\n",
       "      <td>127300.000000</td>\n",
       "      <td>128000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>147133.333333</td>\n",
       "      <td>147633.333333</td>\n",
       "      <td>148700.000000</td>\n",
       "      <td>148900.000000</td>\n",
       "      <td>149566.666667</td>\n",
       "      <td>150366.666667</td>\n",
       "      <td>151733.333333</td>\n",
       "      <td>153466.666667</td>\n",
       "      <td>155100.000000</td>\n",
       "      <td>155850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Albertville</th>\n",
       "      <td>73966.666667</td>\n",
       "      <td>72600.000000</td>\n",
       "      <td>72833.333333</td>\n",
       "      <td>74200.000000</td>\n",
       "      <td>75900.000000</td>\n",
       "      <td>76000.000000</td>\n",
       "      <td>72066.666667</td>\n",
       "      <td>73566.666667</td>\n",
       "      <td>76533.333333</td>\n",
       "      <td>76366.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>84033.333333</td>\n",
       "      <td>84766.666667</td>\n",
       "      <td>86800.000000</td>\n",
       "      <td>88466.666667</td>\n",
       "      <td>89500.000000</td>\n",
       "      <td>90233.333333</td>\n",
       "      <td>91366.666667</td>\n",
       "      <td>92000.000000</td>\n",
       "      <td>92466.666667</td>\n",
       "      <td>92200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arab</th>\n",
       "      <td>83766.666667</td>\n",
       "      <td>81566.666667</td>\n",
       "      <td>81333.333333</td>\n",
       "      <td>82966.666667</td>\n",
       "      <td>84200.000000</td>\n",
       "      <td>84533.333333</td>\n",
       "      <td>81666.666667</td>\n",
       "      <td>83900.000000</td>\n",
       "      <td>87266.666667</td>\n",
       "      <td>87700.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>113366.666667</td>\n",
       "      <td>111700.000000</td>\n",
       "      <td>111600.000000</td>\n",
       "      <td>110166.666667</td>\n",
       "      <td>109433.333333</td>\n",
       "      <td>110900.000000</td>\n",
       "      <td>112233.333333</td>\n",
       "      <td>110033.333333</td>\n",
       "      <td>110100.000000</td>\n",
       "      <td>112000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ardmore</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>140533.333333</td>\n",
       "      <td>139566.666667</td>\n",
       "      <td>140900.000000</td>\n",
       "      <td>143233.333333</td>\n",
       "      <td>143000.000000</td>\n",
       "      <td>144600.000000</td>\n",
       "      <td>143966.666667</td>\n",
       "      <td>142566.666667</td>\n",
       "      <td>143233.333333</td>\n",
       "      <td>141950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Wyoming</th>\n",
       "      <th>Burns</th>\n",
       "      <td>101533.333333</td>\n",
       "      <td>104566.666667</td>\n",
       "      <td>108366.666667</td>\n",
       "      <td>113000.000000</td>\n",
       "      <td>115833.333333</td>\n",
       "      <td>117200.000000</td>\n",
       "      <td>117800.000000</td>\n",
       "      <td>117633.333333</td>\n",
       "      <td>117333.333333</td>\n",
       "      <td>117233.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>168866.666667</td>\n",
       "      <td>161933.333333</td>\n",
       "      <td>160433.333333</td>\n",
       "      <td>162600.000000</td>\n",
       "      <td>163066.666667</td>\n",
       "      <td>164600.000000</td>\n",
       "      <td>164300.000000</td>\n",
       "      <td>168266.666667</td>\n",
       "      <td>171600.000000</td>\n",
       "      <td>170500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Casper</th>\n",
       "      <td>89233.333333</td>\n",
       "      <td>89600.000000</td>\n",
       "      <td>89733.333333</td>\n",
       "      <td>93166.666667</td>\n",
       "      <td>95500.000000</td>\n",
       "      <td>97633.333333</td>\n",
       "      <td>99433.333333</td>\n",
       "      <td>100633.333333</td>\n",
       "      <td>101733.333333</td>\n",
       "      <td>101533.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>175766.666667</td>\n",
       "      <td>177300.000000</td>\n",
       "      <td>181000.000000</td>\n",
       "      <td>182066.666667</td>\n",
       "      <td>182633.333333</td>\n",
       "      <td>183300.000000</td>\n",
       "      <td>182700.000000</td>\n",
       "      <td>184333.333333</td>\n",
       "      <td>185166.666667</td>\n",
       "      <td>184350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cheyenne</th>\n",
       "      <td>116866.666667</td>\n",
       "      <td>120033.333333</td>\n",
       "      <td>121533.333333</td>\n",
       "      <td>123633.333333</td>\n",
       "      <td>125533.333333</td>\n",
       "      <td>126300.000000</td>\n",
       "      <td>126466.666667</td>\n",
       "      <td>128133.333333</td>\n",
       "      <td>128466.666667</td>\n",
       "      <td>129633.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>177466.666667</td>\n",
       "      <td>176733.333333</td>\n",
       "      <td>178766.666667</td>\n",
       "      <td>181700.000000</td>\n",
       "      <td>183266.666667</td>\n",
       "      <td>186766.666667</td>\n",
       "      <td>190666.666667</td>\n",
       "      <td>194433.333333</td>\n",
       "      <td>196500.000000</td>\n",
       "      <td>199100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Evansville</th>\n",
       "      <td>128033.333333</td>\n",
       "      <td>128766.666667</td>\n",
       "      <td>130833.333333</td>\n",
       "      <td>132066.666667</td>\n",
       "      <td>130566.666667</td>\n",
       "      <td>131433.333333</td>\n",
       "      <td>132400.000000</td>\n",
       "      <td>133466.666667</td>\n",
       "      <td>133300.000000</td>\n",
       "      <td>131066.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>296733.333333</td>\n",
       "      <td>305666.666667</td>\n",
       "      <td>309500.000000</td>\n",
       "      <td>307300.000000</td>\n",
       "      <td>303166.666667</td>\n",
       "      <td>300966.666667</td>\n",
       "      <td>304200.000000</td>\n",
       "      <td>309433.333333</td>\n",
       "      <td>309200.000000</td>\n",
       "      <td>309050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pine Bluffs</th>\n",
       "      <td>93733.333333</td>\n",
       "      <td>95066.666667</td>\n",
       "      <td>94633.333333</td>\n",
       "      <td>98066.666667</td>\n",
       "      <td>103233.333333</td>\n",
       "      <td>104600.000000</td>\n",
       "      <td>106500.000000</td>\n",
       "      <td>104066.666667</td>\n",
       "      <td>102233.333333</td>\n",
       "      <td>103566.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>148666.666667</td>\n",
       "      <td>154366.666667</td>\n",
       "      <td>158100.000000</td>\n",
       "      <td>163900.000000</td>\n",
       "      <td>167433.333333</td>\n",
       "      <td>167466.666667</td>\n",
       "      <td>169200.000000</td>\n",
       "      <td>166833.333333</td>\n",
       "      <td>163800.000000</td>\n",
       "      <td>157650.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10730 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            2000q1         2000q2         2000q3  \\\n",
       "State   RegionName                                                 \n",
       "Alabama Adamsville    69033.333333   69166.666667   69800.000000   \n",
       "        Alabaster    122133.333333  123066.666667  123166.666667   \n",
       "        Albertville   73966.666667   72600.000000   72833.333333   \n",
       "        Arab          83766.666667   81566.666667   81333.333333   \n",
       "        Ardmore                NaN            NaN            NaN   \n",
       "...                            ...            ...            ...   \n",
       "Wyoming Burns        101533.333333  104566.666667  108366.666667   \n",
       "        Casper        89233.333333   89600.000000   89733.333333   \n",
       "        Cheyenne     116866.666667  120033.333333  121533.333333   \n",
       "        Evansville   128033.333333  128766.666667  130833.333333   \n",
       "        Pine Bluffs   93733.333333   95066.666667   94633.333333   \n",
       "\n",
       "                            2000q4         2001q1         2001q2  \\\n",
       "State   RegionName                                                 \n",
       "Alabama Adamsville    71966.666667   73466.666667   74000.000000   \n",
       "        Alabaster    123700.000000  123233.333333  125133.333333   \n",
       "        Albertville   74200.000000   75900.000000   76000.000000   \n",
       "        Arab          82966.666667   84200.000000   84533.333333   \n",
       "        Ardmore                NaN            NaN            NaN   \n",
       "...                            ...            ...            ...   \n",
       "Wyoming Burns        113000.000000  115833.333333  117200.000000   \n",
       "        Casper        93166.666667   95500.000000   97633.333333   \n",
       "        Cheyenne     123633.333333  125533.333333  126300.000000   \n",
       "        Evansville   132066.666667  130566.666667  131433.333333   \n",
       "        Pine Bluffs   98066.666667  103233.333333  104600.000000   \n",
       "\n",
       "                            2001q3         2001q4         2002q1  \\\n",
       "State   RegionName                                                 \n",
       "Alabama Adamsville    73333.333333   73100.000000   73333.333333   \n",
       "        Alabaster    127766.666667  127200.000000  127300.000000   \n",
       "        Albertville   72066.666667   73566.666667   76533.333333   \n",
       "        Arab          81666.666667   83900.000000   87266.666667   \n",
       "        Ardmore                NaN            NaN            NaN   \n",
       "...                            ...            ...            ...   \n",
       "Wyoming Burns        117800.000000  117633.333333  117333.333333   \n",
       "        Casper        99433.333333  100633.333333  101733.333333   \n",
       "        Cheyenne     126466.666667  128133.333333  128466.666667   \n",
       "        Evansville   132400.000000  133466.666667  133300.000000   \n",
       "        Pine Bluffs  106500.000000  104066.666667  102233.333333   \n",
       "\n",
       "                            2002q2  ...         2014q2         2014q3  \\\n",
       "State   RegionName                  ...                                 \n",
       "Alabama Adamsville    73133.333333  ...   77066.666667   75966.666667   \n",
       "        Alabaster    128000.000000  ...  147133.333333  147633.333333   \n",
       "        Albertville   76366.666667  ...   84033.333333   84766.666667   \n",
       "        Arab          87700.000000  ...  113366.666667  111700.000000   \n",
       "        Ardmore                NaN  ...  140533.333333  139566.666667   \n",
       "...                            ...  ...            ...            ...   \n",
       "Wyoming Burns        117233.333333  ...  168866.666667  161933.333333   \n",
       "        Casper       101533.333333  ...  175766.666667  177300.000000   \n",
       "        Cheyenne     129633.333333  ...  177466.666667  176733.333333   \n",
       "        Evansville   131066.666667  ...  296733.333333  305666.666667   \n",
       "        Pine Bluffs  103566.666667  ...  148666.666667  154366.666667   \n",
       "\n",
       "                            2014q4         2015q1         2015q2  \\\n",
       "State   RegionName                                                 \n",
       "Alabama Adamsville    71900.000000   71666.666667   73033.333333   \n",
       "        Alabaster    148700.000000  148900.000000  149566.666667   \n",
       "        Albertville   86800.000000   88466.666667   89500.000000   \n",
       "        Arab         111600.000000  110166.666667  109433.333333   \n",
       "        Ardmore      140900.000000  143233.333333  143000.000000   \n",
       "...                            ...            ...            ...   \n",
       "Wyoming Burns        160433.333333  162600.000000  163066.666667   \n",
       "        Casper       181000.000000  182066.666667  182633.333333   \n",
       "        Cheyenne     178766.666667  181700.000000  183266.666667   \n",
       "        Evansville   309500.000000  307300.000000  303166.666667   \n",
       "        Pine Bluffs  158100.000000  163900.000000  167433.333333   \n",
       "\n",
       "                            2015q3         2015q4         2016q1  \\\n",
       "State   RegionName                                                 \n",
       "Alabama Adamsville    73933.333333   73866.666667   74166.666667   \n",
       "        Alabaster    150366.666667  151733.333333  153466.666667   \n",
       "        Albertville   90233.333333   91366.666667   92000.000000   \n",
       "        Arab         110900.000000  112233.333333  110033.333333   \n",
       "        Ardmore      144600.000000  143966.666667  142566.666667   \n",
       "...                            ...            ...            ...   \n",
       "Wyoming Burns        164600.000000  164300.000000  168266.666667   \n",
       "        Casper       183300.000000  182700.000000  184333.333333   \n",
       "        Cheyenne     186766.666667  190666.666667  194433.333333   \n",
       "        Evansville   300966.666667  304200.000000  309433.333333   \n",
       "        Pine Bluffs  167466.666667  169200.000000  166833.333333   \n",
       "\n",
       "                            2016q2    2016q3  \n",
       "State   RegionName                            \n",
       "Alabama Adamsville    74933.333333   74700.0  \n",
       "        Alabaster    155100.000000  155850.0  \n",
       "        Albertville   92466.666667   92200.0  \n",
       "        Arab         110100.000000  112000.0  \n",
       "        Ardmore      143233.333333  141950.0  \n",
       "...                            ...       ...  \n",
       "Wyoming Burns        171600.000000  170500.0  \n",
       "        Casper       185166.666667  184350.0  \n",
       "        Cheyenne     196500.000000  199100.0  \n",
       "        Evansville   309200.000000  309050.0  \n",
       "        Pine Bluffs  163800.000000  157650.0  \n",
       "\n",
       "[10730 rows x 67 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_housing_data_to_quarters():\n",
    "    '''Converts the housing data to quarters and returns it as mean \n",
    "    values in a dataframe. This dataframe should be a dataframe with\n",
    "    columns for 2000q1 through 2016q3, and should have a multi-index\n",
    "    in the shape of [\"State\",\"RegionName\"].\n",
    "    \n",
    "    Note: Quarters are defined in the assignment description, they are\n",
    "    not arbitrary three month periods.\n",
    "    \n",
    "    The resulting dataframe should have 67 columns, and 10,730 rows.\n",
    "    '''\n",
    "    house_data = pd.read_csv('City_Zhvi_AllHomes.txt')\n",
    "    # We delete the months we are not interested in (before 2000-01)\n",
    "    house_data.drop(house_data.iloc[:,6:51], axis=1, inplace=True)\n",
    "    # Now we calculate the means of the quarters\n",
    "    for i in range(6,203,3):\n",
    "        house_data.iloc[:,i] = house_data.iloc[:,[i,i+1,i+2]].mean(axis=1)\n",
    "    # We only have two columns for the last quarter, so we calculate it with these two\n",
    "    house_data.iloc[:,-2] = house_data.iloc[:,[-1,-2]].mean(axis=1)\n",
    "    # We need to get rid now of those columns which do not contain the means (the quarters)\n",
    "    house_data.drop(house_data.iloc[:,7:206:3], axis=1, inplace=True)\n",
    "    house_data.drop(house_data.iloc[:,7:138:2], axis=1, inplace=True)\n",
    "    # Now we map the two letter code for the states to complete names using the dict 'States' given at the very beginning \n",
    "    # of the project \n",
    "    house_data['State'] = house_data['State'].map(states)\n",
    "    # Now we set the multiindex as requested in the exercise\n",
    "    house_data.set_index(['State', 'RegionName'], inplace=True)\n",
    "    house_data.sort_index(inplace=True)\n",
    "    # We get rid of the columns which are not necessary (RegionID, Metro, CountyName, SizeRank)\n",
    "    house_data.drop(house_data.iloc[:,0:4], axis=1, inplace=True)\n",
    "    # We need to rename the columns using the quarters notation. To do so we create a list:\n",
    "    columns = []\n",
    "    for i in range(2000,2017):\n",
    "        for j in range(1,5):\n",
    "            columnname = str(i)+ 'q' + str(j)\n",
    "            columns.append(columnname)\n",
    "    # We have an extra column name (2016q4) which we need to remove \n",
    "    del columns[-1]\n",
    "    # Now we can set this list as the dataframe's columns names\n",
    "    house_data.columns = columns\n",
    "    \n",
    "    return house_data\n",
    "\n",
    "convert_housing_data_to_quarters()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "umich_part_id": "026",
    "umich_partlist_id": "004"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 0.002724063704761164, 'university town')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_ttest():\n",
    "    '''First creates new data showing the decline or growth of housing prices\n",
    "    between the recession start and the recession bottom. Then runs a ttest\n",
    "    comparing the university town values to the non-university towns values, \n",
    "    return whether the alternative hypothesis (that the two groups are the same)\n",
    "    is true or not as well as the p-value of the confidence. \n",
    "    \n",
    "    Return the tuple (different, p, better) where different=True if the t-test is\n",
    "    True at a p<0.01 (we reject the null hypothesis), or different=False if \n",
    "    otherwise (we cannot reject the null hypothesis). The variable p should\n",
    "    be equal to the exact p value returned from scipy.stats.ttest_ind(). The\n",
    "    value for better should be either \"university town\" or \"non-university town\"\n",
    "    depending on which has a lower mean price ratio (which is equivilent to a\n",
    "    reduced market loss).'''\n",
    "    \n",
    "    # From the project formulation at the beginning:\n",
    "    \n",
    "    # HYPOTHESIS: University towns have their mean housing prices less affected by recessions. Run a t-test to \n",
    "    # compare the ratio of the mean price of houses both in university towns and in non uni towns the quarter before \n",
    "    # the recession starts compared to the recession bottom. (price_ratio=quarter_before_recession/recession_bottom).\n",
    "    \n",
    "    # The null hypothesis is: The mean housing price of University towns and Non- University towns is same .\n",
    "    # Alternate Hypothesis: The mean housing price of University towns and Non- University towns is different.\n",
    "    \n",
    "    # --------------------------------------------------------------------------------------------------------------------------\n",
    "    # Logic of the hypothesis testing explained in detail:\n",
    "    # --------------------------------------------------------------------------------------------------------------------------\n",
    "    # Statement hypothesis (alternate hypothesis) – University towns have their mean housing price less affected by recessions.\n",
    "\n",
    "    # Question – How do we measure the effect of a recession on the mean housing price in university towns? \n",
    "    # Answer – Using price ratio, we measure the effect of a recession on the mean housing price in university towns\n",
    "\n",
    "    # Mean price ratio of university towns versus mean price ratio of non-university towns:\n",
    "\n",
    "    # SCENARIO 1: If p-value > significance threshold (say 0.01), then we cannot reject the null hypothesis of identical averages.\n",
    "\n",
    "    # Null hypothesis is always of identical average scores –> mean [price ratio] doesn’t differ significantly across samples \n",
    "    # ‘or’ is more or less the same between university towns and non-university towns –> Not being able to reject the null \n",
    "    # hypothesis implies that it is more credible (means are same across both samples) which in turn implies that the effects \n",
    "    # of recession are more or less the same in all towns, regardless of them being university towns or non-university towns.\n",
    "\n",
    "    # SCENARIO 2: If p-value < significance threshold (say 0.01), then we can reject the null hypothesis of identical averages.\n",
    "\n",
    "    # Alternate hypothesis is always of different average scores –> mean [price ratio] differs significantly across samples \n",
    "    # –> Being able to reject the null hypothesis implies that it is less credible than the alternative hypothesis (which is the\n",
    "    # opposite of null hypothesis; i.e. means are different across both samples) which in turn implies that the effects of \n",
    "    # recession are different for university towns and in non-university towns.\n",
    "\n",
    "    # How do we add in the factor of ‘less effect’ or ‘more effect’?\n",
    "    # Let’s say p-value indeed is less than alpha. Hence, we reject the null hypothesis –> effects of recession are different for\n",
    "    # university towns and non-university towns. Then, we compare mean price ratio = mean of price ratio column for both categories\n",
    "    # of towns. We find that ‘mean price ratio for university towns’ < ‘mean price ratio for non-university towns’ (comparison of \n",
    "    # 1 number to another number)\n",
    "\n",
    "    # Recall that mean price ratio = (mean price during quarter before recession) / (mean price during recession bottom). Because \n",
    "    # mean price ratio for university towns is less than that for non-university towns, we deduce that effects of recession on \n",
    "    # mean housing price are more pronounced in non-university towns than university towns.\n",
    "    # --------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # We have to use a few of the previous functions to work in this one, but we will assign shorter names to them\n",
    "    house_data = convert_housing_data_to_quarters()\n",
    "    rec_sta = get_recession_start()\n",
    "    rec_bot = get_recession_bottom()\n",
    "    uni_towns = get_list_of_university_towns().set_index(['State', 'RegionName']) # We set the index the same way as the house_data df\n",
    "    \n",
    "    # Now we are able to calculate the ratio which is defined at the beginning of the project as:\n",
    "    # (price_ratio=quarter_before_recession/recession_bottom). If the Ratio is > 1 that means that the prices were higher \n",
    "    # before the recession, if it is < 1, that means the prices got higher during the recession (prices not affected by the recession, \n",
    "    # which is equivilent to a reduced market loss).\n",
    "    \n",
    "    # We use the get_loc attribute to find the column index, because we need to substract one to this index to get the index of\n",
    "    # the quarter BEFORE the recession starts (the function get_recession_start() gives us the quarter WHEN IT STARTS, as the \n",
    "    # name states)\n",
    "    bef_rec_sta_ind = house_data.columns.get_loc(rec_sta)-1\n",
    "    house_data['Ratio'] = house_data.iloc[:,bef_rec_sta_ind]/house_data[rec_bot]\n",
    "    # We just need the ratio for the purpose of this exercise, so we will reduce our house df to this column.\n",
    "    # It is also important to note that this has to be converted to a frame because it will be considered a Series. We need it \n",
    "    # to be a Dataframe beacuse we have to merge it later with the Uni towns df.\n",
    "    # We get rid of NaN too to finish this task\n",
    "    house_data = house_data.iloc[:,-1].to_frame().dropna()\n",
    "    \n",
    "    # The price ratio obtained in this manner should now be divided into university towns and non-university towns which is \n",
    "    # then supplied into the ttest_ind function with the argument nan_policy=\"omit\".\n",
    "    # But first we need to check the cities in the house data which are uni towns, by using inner merging\n",
    "    uni_ratios = house_data.merge(uni_towns, right_index=True, left_index=True, how='inner')\n",
    "    # Now the ratios of those cities which are not uni\n",
    "    nonuni_ratios = house_data.drop(uni_ratios.index)\n",
    "    \n",
    "    p_value = ttest_ind(uni_ratios.values, nonuni_ratios.values).pvalue\n",
    "    \n",
    "    # We state the variable different to evaluate the hypothesis testing\n",
    "    if p_value < 0.01:\n",
    "        different=True\n",
    "    else:\n",
    "        different=False\n",
    "        \n",
    "    # Better depending on which one is LOWER! Remember prices go up during a recession so lower is better.\n",
    "    if uni_ratios.mean().values < nonuni_ratios.mean().values:\n",
    "        better='university town'\n",
    "    else:\n",
    "        better='non-university town'\n",
    "    \n",
    "    return (different, p_value[0], better)\n",
    "\n",
    "run_ttest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-data-analysis",
   "graded_item_id": "Il9Fx",
   "launcher_item_id": "TeDW0",
   "part_id": "WGlun"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "umich": {
   "id": "Assignment 4",
   "version": "1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
